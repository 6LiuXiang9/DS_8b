from unsloth import FastLanguageModel
import torch
import os
import json
import random
from bert_score import BERTScorer
import pandas as pd
from tqdm import tqdm
import numpy as np
import time

# Print current working directory
print(f"Current working directory: {os.getcwd()}")

# Model configuration
max_seq_length = 2048
load_in_4bit = True

# Paths - æ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ­£
model_path = "E:/DS_8b/DeepSeek-R1-Medical-COT_710"  # ä¿®æ­£ä¸ºå®é™…æ¨¡å‹è·¯å¾„
test_data_path = "Test_medical_data_with_cot_reasoner_100.json"  # å½“å‰ç›®å½•ä¸‹çš„æµ‹è¯•æ•°æ®

print(f"Loading model from: {model_path}")
print(f"Loading test data from: {test_data_path}")

# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if not os.path.exists(model_path):
    print(f"âŒ Model path does not exist: {model_path}")
    exit(1)

if not os.path.exists(test_data_path):
    print(f"âŒ Test data path does not exist: {test_data_path}")
    exit(1)

# Load the fine-tuned model and tokenizer
try:
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_path,
        max_seq_length=max_seq_length,
        load_in_4bit=load_in_4bit,
    )
    print("âœ… Model and tokenizer loaded successfully!")
except Exception as e:
    print(f"âŒ Error loading model: {e}")
    exit(1)

# Set the model to inference mode
FastLanguageModel.for_inference(model)
print("âœ… Model prepared for inference")

# Load test dataset
try:
    with open(test_data_path, 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    print(f"âœ… Loaded test dataset: {len(test_data)} samples")
except Exception as e:
    print(f"âŒ Error loading test data: {e}")
    exit(1)

# Setup prompt template (consistent with training)
test_prompt_template = """Below is an instruction that describes a task, paired with an input that provides further context.
Write a response that appropriately completes the request.
Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.

### Instruction:
You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.
Please answer the following medical question.

### Question:
{}

### Response:
"""

# Initialize BERTScore for Chinese - ä¿®å¤ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
print("Initializing BERTScorer...")
scorer = None

# å°è¯•å¤šç§åˆå§‹åŒ–æ–¹å¼
init_methods = [
    # æ–¹æ³•1: ä½¿ç”¨langå‚æ•°
    lambda: BERTScorer(lang='zh'),
    # æ–¹æ³•2: ä½¿ç”¨model_typeå‚æ•°
    lambda: BERTScorer(model_type='bert-base-chinese'),
    # æ–¹æ³•3: ä½¿ç”¨é»˜è®¤ä¸­æ–‡æ¨¡å‹
    lambda: BERTScorer(model_type='hfl/chinese-bert-wwm-ext'),
    # æ–¹æ³•4: æœ€åŸºæœ¬çš„åˆå§‹åŒ–
    lambda: BERTScorer()
]

for i, init_method in enumerate(init_methods):
    try:
        print(f"å°è¯•åˆå§‹åŒ–æ–¹æ³• {i + 1}...")
        scorer = init_method()
        print(f"âœ… BERTScorer initialized successfully with method {i + 1}")
        break
    except Exception as e:
        print(f"âŒ Method {i + 1} failed: {e}")
        continue

if scorer is None:
    print("âŒ All BERTScorer initialization methods failed")
    print("è¯·æ£€æŸ¥bert-scoreåŒ…çš„å®‰è£…: pip install bert-score")
    exit(1)

# Setup device
if torch.cuda.is_available():
    device = "cuda"
    model = model.to(device)
    print("âœ… Using CUDA for inference")
else:
    device = "cpu"
    print("âš ï¸ Using CPU for inference (this might be slow)")


def generate_model_response(question, max_retries=3):
    """ç”Ÿæˆæ¨¡å‹å›ç­”ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    for attempt in range(max_retries):
        try:
            formatted_input = test_prompt_template.format(question)
            inputs = tokenizer([formatted_input], return_tensors="pt")

            if device == "cuda":
                inputs = inputs.to(device)

            with torch.no_grad():
                outputs = model.generate(
                    input_ids=inputs.input_ids,
                    attention_mask=inputs.attention_mask,
                    max_new_tokens=1200,
                    temperature=0.7,
                    top_p=0.9,
                    use_cache=True,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                )

            full_response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
            response_only = full_response.split("### Response:")[1].strip()

            # å¤„ç†CoTæ€ç»´é“¾æ ¼å¼
            if "</think>" in response_only:
                # æå–æ€ç»´é“¾å’Œæœ€ç»ˆå›ç­”
                parts = response_only.split("</think>")
                if len(parts) > 1:
                    think_part = parts[0].replace("<think>", "").strip()
                    final_answer = parts[1].strip()
                    return {
                        "full_response": response_only,
                        "thinking": think_part,
                        "final_answer": final_answer
                    }

            # å¦‚æœæ²¡æœ‰æ€ç»´é“¾æ ¼å¼ï¼Œè¿”å›æ•´ä¸ªå›ç­”
            return {
                "full_response": response_only,
                "thinking": "",
                "final_answer": response_only
            }

        except Exception as e:
            print(f"âš ï¸ Attempt {attempt + 1} failed: {e}")
            if attempt == max_retries - 1:
                return {
                    "full_response": f"Error generating response: {e}",
                    "thinking": "",
                    "final_answer": f"Error: {e}"
                }
            time.sleep(1)


def safe_bert_score(candidates, references, max_retries=3):
    """å®‰å…¨çš„BERTScoreè®¡ç®—ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    for attempt in range(max_retries):
        try:
            # æ¸…ç†è¾“å…¥æ–‡æœ¬
            clean_candidates = [str(c).strip() if c else "" for c in candidates]
            clean_references = [str(r).strip() if r else "" for r in references]

            # å¦‚æœä»»ä½•ä¸€ä¸ªä¸ºç©ºï¼Œè¿”å›é»˜è®¤åˆ†æ•°
            if not any(clean_candidates) or not any(clean_references):
                return [0.0] * len(candidates), [0.0] * len(candidates), [0.0] * len(candidates)

            P, R, F1 = scorer.score(clean_candidates, clean_references)
            return P, R, F1

        except Exception as e:
            print(f"âš ï¸ BERTScore calculation attempt {attempt + 1} failed: {e}")
            if attempt == max_retries - 1:
                print("ä½¿ç”¨é»˜è®¤åˆ†æ•°...")
                return [0.0] * len(candidates), [0.0] * len(candidates), [0.0] * len(candidates)
            time.sleep(1)


def calculate_comprehensive_bertscore(model_output, reference_data):
    """
    è®¡ç®—å…¨é¢çš„BERTScoreè¯„ä¼°ï¼š
    1. Response BERTScore (æ¨¡å‹æœ€ç»ˆå›ç­” vs å‚è€ƒå›ç­”)
    2. CoT BERTScore (æ¨¡å‹æ€ç»´é“¾ vs å‚è€ƒæ€ç»´é“¾)
    3. Combined BERTScore-L (æ¨¡å‹å®Œæ•´å›ç­” vs å‚è€ƒå®Œæ•´å†…å®¹)
    """

    # å‡†å¤‡æ•°æ®
    model_response = model_output["final_answer"] or ""
    model_cot = model_output["thinking"] or ""
    model_combined = f"{model_cot} {model_response}".strip()

    reference_response = reference_data["Response"] or ""
    reference_cot = reference_data.get("Complex_CoT", "") or ""
    reference_combined = f"{reference_cot} {reference_response}".strip()

    scores = {}

    try:
        # 1. Response BERTScore
        if model_response and reference_response:
            P_resp, R_resp, F1_resp = safe_bert_score([model_response], [reference_response])
            scores['response'] = {
                'precision': float(P_resp[0]) if len(P_resp) > 0 else 0.0,
                'recall': float(R_resp[0]) if len(R_resp) > 0 else 0.0,
                'f1': float(F1_resp[0]) if len(F1_resp) > 0 else 0.0
            }
        else:
            scores['response'] = {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}

        # 2. CoT BERTScore
        if model_cot and reference_cot:
            P_cot, R_cot, F1_cot = safe_bert_score([model_cot], [reference_cot])
            scores['cot'] = {
                'precision': float(P_cot[0]) if len(P_cot) > 0 else 0.0,
                'recall': float(R_cot[0]) if len(R_cot) > 0 else 0.0,
                'f1': float(F1_cot[0]) if len(F1_cot) > 0 else 0.0
            }
        else:
            # å¦‚æœCoTä¸ºç©ºï¼Œç»™äºˆé»˜è®¤åˆ†æ•°
            scores['cot'] = {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}

        # 3. Combined BERTScore-L (æ•´ä½“è¯„ä¼°)
        if model_combined and reference_combined:
            P_comb, R_comb, F1_comb = safe_bert_score([model_combined], [reference_combined])
            scores['combined'] = {
                'precision': float(P_comb[0]) if len(P_comb) > 0 else 0.0,
                'recall': float(R_comb[0]) if len(R_comb) > 0 else 0.0,
                'f1': float(F1_comb[0]) if len(F1_comb) > 0 else 0.0
            }
        else:
            scores['combined'] = {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}

    except Exception as e:
        print(f"âŒ Error calculating comprehensive BERTScore: {e}")
        # è¿”å›é»˜è®¤åˆ†æ•°
        scores = {
            'response': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0},
            'cot': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0},
            'combined': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}
        }

    return scores


def evaluate_model_on_test_set(num_samples=None, random_sample=False):
    """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½"""

    # ç¡®å®šæµ‹è¯•æ ·æœ¬
    if num_samples is None:
        test_samples = test_data
        print(f"ğŸ“Š ä½¿ç”¨å®Œæ•´æµ‹è¯•é›†è¿›è¡Œè¯„ä¼°: {len(test_samples)} ä¸ªæ ·æœ¬")
    else:
        if random_sample:
            test_samples = random.sample(test_data, min(num_samples, len(test_data)))
            print(f"ğŸ“Š éšæœºé€‰æ‹© {len(test_samples)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°")
        else:
            test_samples = test_data[:num_samples]
            print(f"ğŸ“Š ä½¿ç”¨å‰ {len(test_samples)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°")

    results = []

    # ç”¨äºæ”¶é›†æ‰€æœ‰åˆ†æ•°ä»¥è®¡ç®—å¹³å‡å€¼
    all_response_scores = []
    all_cot_scores = []
    all_combined_scores = []

    print("\nğŸš€ å¼€å§‹ç”Ÿæˆæ¨¡å‹å›ç­”å¹¶è®¡ç®—BERTScore...")

    for i, sample in enumerate(tqdm(test_samples, desc="è¯„ä¼°è¿›è¡Œä¸­", ncols=100)):
        try:
            question = sample["Question"]
            reference_response = sample["Response"]
            reference_cot = sample.get("Complex_CoT", "")

            # ç”Ÿæˆæ¨¡å‹å›ç­”
            model_output = generate_model_response(question)

            # è®¡ç®—ä¸‰ç§BERTScore
            bert_scores = calculate_comprehensive_bertscore(model_output, sample)

            # æ”¶é›†åˆ†æ•°ç”¨äºç»Ÿè®¡
            all_response_scores.append(bert_scores['response']['f1'])
            all_cot_scores.append(bert_scores['cot']['f1'])
            all_combined_scores.append(bert_scores['combined']['f1'])

            # ä¿å­˜è¯¦ç»†ç»“æœ
            result = {
                'sample_id': i,
                'question': question,
                'reference_response': reference_response,
                'reference_cot': reference_cot,
                'model_full_response': model_output["full_response"],
                'model_thinking': model_output["thinking"],
                'model_final_answer': model_output["final_answer"],

                # Response BERTScore
                'response_bert_precision': bert_scores['response']['precision'],
                'response_bert_recall': bert_scores['response']['recall'],
                'response_bert_f1': bert_scores['response']['f1'],

                # CoT BERTScore
                'cot_bert_precision': bert_scores['cot']['precision'],
                'cot_bert_recall': bert_scores['cot']['recall'],
                'cot_bert_f1': bert_scores['cot']['f1'],

                # Combined BERTScore-L
                'combined_bert_precision': bert_scores['combined']['precision'],
                'combined_bert_recall': bert_scores['combined']['recall'],
                'combined_bert_f1': bert_scores['combined']['f1'],
            }

            results.append(result)

            # æ¯10ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if (i + 1) % 10 == 0:
                avg_resp = np.mean([r['response_bert_f1'] for r in results])
                avg_cot = np.mean([r['cot_bert_f1'] for r in results])
                avg_comb = np.mean([r['combined_bert_f1'] for r in results])
                print(
                    f"\nğŸ“Š å·²å¤„ç† {i + 1} ä¸ªæ ·æœ¬ - å½“å‰å¹³å‡F1: Response={avg_resp:.3f}, CoT={avg_cot:.3f}, Combined={avg_comb:.3f}")

        except Exception as e:
            print(f"âŒ Error processing sample {i}: {e}")
            continue

    print(f"âœ… æˆåŠŸè¯„ä¼° {len(results)} ä¸ªæ ·æœ¬")

    if len(results) == 0:
        print("âŒ æ²¡æœ‰æˆåŠŸè¯„ä¼°ä»»ä½•æ ·æœ¬ï¼Œè¯„ä¼°ç»ˆæ­¢")
        return None, None

    # è®¡ç®—ä¸‰ä¸ªç»´åº¦çš„ç»Ÿè®¡ä¿¡æ¯
    stats = calculate_comprehensive_statistics(all_response_scores, all_cot_scores, all_combined_scores)

    return results, stats


def calculate_comprehensive_statistics(response_scores, cot_scores, combined_scores):
    """è®¡ç®—ä¸‰ä¸ªç»´åº¦çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""

    def calc_stats(scores):
        scores_array = np.array(scores)
        return {
            'mean': float(np.mean(scores_array)),
            'std': float(np.std(scores_array)),
            'min': float(np.min(scores_array)),
            'max': float(np.max(scores_array)),
            'median': float(np.median(scores_array)),
            'q25': float(np.percentile(scores_array, 25)),
            'q75': float(np.percentile(scores_array, 75))
        }

    stats = {
        'total_samples': len(response_scores),
        'response_f1_stats': calc_stats(response_scores),
        'cot_f1_stats': calc_stats(cot_scores),
        'combined_f1_stats': calc_stats(combined_scores),

        # å¹³å‡åˆ†æ•°ï¼ˆå…³é”®æŒ‡æ ‡ï¼‰
        'average_scores': {
            'response_f1_mean': float(np.mean(response_scores)),
            'cot_f1_mean': float(np.mean(cot_scores)),
            'combined_f1_mean': float(np.mean(combined_scores))
        }
    }

    return stats


def analyze_performance_by_categories(results):
    """æŒ‰ä¸åŒç±»åˆ«åˆ†ææ€§èƒ½"""
    if not results:
        return

    def categorize_score(score):
        if score >= 0.8:
            return 'Excellent (â‰¥0.8)'
        elif score >= 0.6:
            return 'Good (0.6-0.8)'
        elif score >= 0.4:
            return 'Fair (0.4-0.6)'
        else:
            return 'Poor (<0.4)'

    categories = {
        'Response': {'Excellent (â‰¥0.8)': 0, 'Good (0.6-0.8)': 0, 'Fair (0.4-0.6)': 0, 'Poor (<0.4)': 0},
        'CoT': {'Excellent (â‰¥0.8)': 0, 'Good (0.6-0.8)': 0, 'Fair (0.4-0.6)': 0, 'Poor (<0.4)': 0},
        'Combined': {'Excellent (â‰¥0.8)': 0, 'Good (0.6-0.8)': 0, 'Fair (0.4-0.6)': 0, 'Poor (<0.4)': 0}
    }

    for result in results:
        # Responseåˆ†ç±»
        resp_category = categorize_score(result['response_bert_f1'])
        categories['Response'][resp_category] += 1

        # CoTåˆ†ç±»
        cot_category = categorize_score(result['cot_bert_f1'])
        categories['CoT'][cot_category] += 1

        # Combinedåˆ†ç±»
        comb_category = categorize_score(result['combined_bert_f1'])
        categories['Combined'][comb_category] += 1

    print("\nğŸ“Š ä¸‰ç»´åº¦æ€§èƒ½åˆ†å¸ƒåˆ†æ:")
    print("=" * 80)

    for dimension, cats in categories.items():
        print(f"\nğŸ¯ {dimension} BERTScore F1 åˆ†å¸ƒ:")
        print("-" * 50)
        total = sum(cats.values())
        for category, count in cats.items():
            percentage = count / total * 100 if total > 0 else 0
            print(f"  {category}: {count:3d} æ ·æœ¬ ({percentage:5.1f}%)")


def display_comprehensive_results(results, stats):
    """æ˜¾ç¤ºå…¨é¢çš„è¯„ä¼°ç»“æœ"""
    print("\n" + "=" * 80)
    print("ğŸ¯ DeepSeek-R1-Medical-COT_710 å…¨é¢è¯„ä¼°ç»“æœ")
    print("=" * 80)

    if stats:
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {stats['total_samples']}")

        # æ˜¾ç¤ºä¸‰ä¸ªç»´åº¦çš„å¹³å‡F1åˆ†æ•°
        avg_scores = stats['average_scores']
        print(f"\nğŸ† å¹³å‡F1åˆ†æ•°æ€»è§ˆ:")
        print(f"{'ç»´åº¦':<15} {'å¹³å‡F1åˆ†æ•°':<12} {'è¡¨ç°ç­‰çº§'}")
        print("-" * 45)

        def get_performance_level(score):
            if score >= 0.8:
                return "ä¼˜ç§€ â­â­â­"
            elif score >= 0.6:
                return "è‰¯å¥½ â­â­"
            elif score >= 0.4:
                return "ä¸€èˆ¬ â­"
            else:
                return "éœ€æ”¹è¿›"

        print(
            f"{'Response':<15} {avg_scores['response_f1_mean']:<12.4f} {get_performance_level(avg_scores['response_f1_mean'])}")
        print(f"{'CoT':<15} {avg_scores['cot_f1_mean']:<12.4f} {get_performance_level(avg_scores['cot_f1_mean'])}")
        print(
            f"{'Combined':<15} {avg_scores['combined_f1_mean']:<12.4f} {get_performance_level(avg_scores['combined_f1_mean'])}")

        # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“ˆ è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯:")
        print(f"{'ç»´åº¦':<12} {'å‡å€¼':<8} {'æ ‡å‡†å·®':<8} {'ä¸­ä½æ•°':<8} {'æœ€å°å€¼':<8} {'æœ€å¤§å€¼':<8}")
        print("-" * 65)

        dimensions = [
            ('Response', stats['response_f1_stats']),
            ('CoT', stats['cot_f1_stats']),
            ('Combined', stats['combined_f1_stats'])
        ]

        for dim_name, dim_stats in dimensions:
            print(f"{dim_name:<12} "
                  f"{dim_stats['mean']:.4f}   "
                  f"{dim_stats['std']:.4f}   "
                  f"{dim_stats['median']:.4f}   "
                  f"{dim_stats['min']:.4f}   "
                  f"{dim_stats['max']:.4f}")

    # æ€§èƒ½åˆ†å¸ƒåˆ†æ
    if results:
        analyze_performance_by_categories(results)


def save_comprehensive_results(results, stats, filename="comprehensive_evaluation_results_01.json"):
    """ä¿å­˜å…¨é¢çš„è¯„ä¼°ç»“æœ"""
    output_data = {
        "evaluation_info": {
            "model_path": model_path,
            "test_data_path": test_data_path,
            "evaluation_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_test_samples": len(test_data),
            "evaluated_samples": len(results) if results else 0,
            "evaluation_dimensions": ["Response", "CoT", "Combined"]
        },
        "summary_statistics": stats,
        "detailed_results": results
    }

    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ å…¨é¢è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    except Exception as e:
        print(f"âŒ ä¿å­˜ç»“æœæ—¶å‡ºé”™: {e}")


def create_comprehensive_summary_report(stats):
    """åˆ›å»ºå…¨é¢çš„è¯„ä¼°æ‘˜è¦æŠ¥å‘Š"""
    if not stats:
        return

    avg_scores = stats['average_scores']

    summary = f"""
ğŸ“‹ DeepSeek-R1-Medical-COT_710 å…¨é¢è¯„ä¼°æ‘˜è¦æŠ¥å‘Š
{'=' * 60}

ğŸ“Š è¯„ä¼°æ¦‚å†µ:
   - æµ‹è¯•æ ·æœ¬æ•°: {stats['total_samples']}
   - è¯„ä¼°ç»´åº¦: Response, CoT, Combined
   - è¯„ä¼°æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}

ğŸ¯ ä¸‰ç»´åº¦å¹³å‡F1åˆ†æ•°:
   - Response F1:  {avg_scores['response_f1_mean']:.4f}
   - CoT F1:       {avg_scores['cot_f1_mean']:.4f}  
   - Combined F1:  {avg_scores['combined_f1_mean']:.4f}

ğŸ“ˆ æ•´ä½“æ€§èƒ½è¯„ä¼°:
   - Response: {'ä¼˜ç§€' if avg_scores['response_f1_mean'] >= 0.8 else 'è‰¯å¥½' if avg_scores['response_f1_mean'] >= 0.6 else 'éœ€æ”¹è¿›'}
   - CoT: {'ä¼˜ç§€' if avg_scores['cot_f1_mean'] >= 0.8 else 'è‰¯å¥½' if avg_scores['cot_f1_mean'] >= 0.6 else 'éœ€æ”¹è¿›'}
   - Combined: {'ä¼˜ç§€' if avg_scores['combined_f1_mean'] >= 0.8 else 'è‰¯å¥½' if avg_scores['combined_f1_mean'] >= 0.6 else 'éœ€æ”¹è¿›'}

ğŸ’¡ æ”¹è¿›å»ºè®®:
   - Responseè¡¨ç°: {'ç»§ç»­ä¿æŒ' if avg_scores['response_f1_mean'] >= 0.7 else 'å¢å¼ºå›ç­”è´¨é‡è®­ç»ƒ'}
   - CoTè¡¨ç°: {'ç»§ç»­ä¿æŒ' if avg_scores['cot_f1_mean'] >= 0.7 else 'åŠ å¼ºæ€ç»´é“¾è®­ç»ƒ'}
   - æ•´ä½“è¡¨ç°: {'æ¨¡å‹è¡¨ç°ä¼˜å¼‚ï¼Œå¯æŠ•å…¥ä½¿ç”¨' if avg_scores['combined_f1_mean'] >= 0.75 else 'å»ºè®®ç»§ç»­ä¼˜åŒ–è®­ç»ƒ'}

ğŸ” å…³é”®å‘ç°:
   - æœ€å¼ºé¡¹: {'Response' if avg_scores['response_f1_mean'] == max(avg_scores.values()) else 'CoT' if avg_scores['cot_f1_mean'] == max(avg_scores.values()) else 'Combined'}
   - æœ€éœ€è¦æ”¹è¿›: {'Response' if avg_scores['response_f1_mean'] == min(avg_scores.values()) else 'CoT' if avg_scores['cot_f1_mean'] == min(avg_scores.values()) else 'Combined'}
"""

    print(summary)

    # ä¿å­˜æ‘˜è¦æŠ¥å‘Š
    try:
        with open("comprehensive_evaluation_summary_01.txt", 'w', encoding='utf-8') as f:
            f.write(summary)
        print("ğŸ“„ å…¨é¢æ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜åˆ°: comprehensive_evaluation_summary_01.txt")
    except Exception as e:
        print(f"âŒ ä¿å­˜æ‘˜è¦æŠ¥å‘Šæ—¶å‡ºé”™: {e}")


# ä¸»è¯„ä¼°æµç¨‹
if __name__ == "__main__":
    print("\nğŸ¯ å¼€å§‹å…¨é¢è¯„ä¼° DeepSeek-R1-Medical-COT_710 æ¨¡å‹")
    print("ğŸ“ ä¸‰ç»´åº¦BERTScoreè¯„ä¼°: Response + CoT + Combined")
    print("=" * 80)

    # ç”¨æˆ·é€‰æ‹©è¯„ä¼°è§„æ¨¡
    print("\nğŸ“Š è¯„ä¼°é€‰é¡¹:")
    print("1. å®Œæ•´æµ‹è¯•é›†è¯„ä¼° (æ‰€æœ‰100ä¸ªæ ·æœ¬)")
    print("2. å¿«é€Ÿè¯„ä¼° (éšæœº30ä¸ªæ ·æœ¬)")
    print("3. å°è§„æ¨¡æµ‹è¯• (å‰10ä¸ªæ ·æœ¬)")
    print("4. è‡ªå®šä¹‰æ ·æœ¬æ•°é‡")

    choice = input("\nè¯·é€‰æ‹©è¯„ä¼°æ–¹å¼ (1/2/3/4): ").strip()

    if choice == "1":
        print("ğŸš€ å¼€å§‹å®Œæ•´æµ‹è¯•é›†è¯„ä¼°...")
        results, stats = evaluate_model_on_test_set()

    elif choice == "2":
        print("ğŸš€ å¼€å§‹å¿«é€Ÿè¯„ä¼°...")
        results, stats = evaluate_model_on_test_set(num_samples=30, random_sample=True)

    elif choice == "3":
        print("ğŸš€ å¼€å§‹å°è§„æ¨¡æµ‹è¯•...")
        results, stats = evaluate_model_on_test_set(num_samples=10, random_sample=False)

    elif choice == "4":
        try:
            num_samples = int(input("è¯·è¾“å…¥æ ·æœ¬æ•°é‡: "))
            random_choice = input("æ˜¯å¦éšæœºé€‰æ‹©æ ·æœ¬? (y/n): ").strip().lower() == 'y'
            print(f"ğŸš€ å¼€å§‹è‡ªå®šä¹‰è¯„ä¼° ({num_samples} ä¸ªæ ·æœ¬)...")
            results, stats = evaluate_model_on_test_set(num_samples=num_samples, random_sample=random_choice)
        except ValueError:
            print("âŒ è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å¿«é€Ÿè¯„ä¼°...")
            results, stats = evaluate_model_on_test_set(num_samples=30, random_sample=True)

    else:
        print("âŒ é€‰æ‹©æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å¿«é€Ÿè¯„ä¼°...")
        results, stats = evaluate_model_on_test_set(num_samples=30, random_sample=True)

    # æ˜¾ç¤ºå’Œä¿å­˜ç»“æœ
    if results and stats:
        display_comprehensive_results(results, stats)
        save_comprehensive_results(results, stats)
        create_comprehensive_summary_report(stats)

        # æ˜¾ç¤ºæœ€ç»ˆçš„ä¸‰ç»´åº¦å¹³å‡F1åˆ†æ•°
        avg_scores = stats['average_scores']
        print(f"\nğŸ¯ æœ€ç»ˆä¸‰ç»´åº¦å¹³å‡F1åˆ†æ•°:")
        print(f"Response F1 å¹³å‡åˆ†: {avg_scores['response_f1_mean']:.4f}")
        print(f"CoT F1 å¹³å‡åˆ†:      {avg_scores['cot_f1_mean']:.4f}")
        print(f"Combined F1 å¹³å‡åˆ†: {avg_scores['combined_f1_mean']:.4f}")

    else:
        print("âŒ è¯„ä¼°å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹å’Œæ•°æ®è·¯å¾„")

    print("\nâœ… å…¨é¢è¯„ä¼°å®Œæˆï¼")
    print("\nğŸ“‹ è¯„ä¼°æ–‡ä»¶è¾“å‡º:")
    print("   - comprehensive_evaluation_results_01.json (è¯¦ç»†ç»“æœ)")
    print("   - comprehensive_evaluation_summary_01.txt (æ‘˜è¦æŠ¥å‘Š)")
    print("\nğŸ¯ ä¸‰ç»´åº¦F1å¹³å‡åˆ†æ•°å·²è®¡ç®—å¹¶å±•ç¤º")

    # ç­‰å¾…ç”¨æˆ·æŒ‰é”®ç»“æŸ
    input("\næŒ‰ä»»æ„é”®ç»“æŸç¨‹åº...")